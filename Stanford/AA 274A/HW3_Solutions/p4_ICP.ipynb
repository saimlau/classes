{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ICP For Point-Cloud Registration\n",
        "\n",
        "In this problem you will be implementing the ICP (Iterative Closest Point) algorithm to register two point clouds. Registration refers to aligning to be coherent. In robotics, this is needed when a lidar scan from different parts of a room for instance need to be joined together to create a full map of the environment.You can see some examples in [this paper](http://redwood-data.org/indoor_lidar_rgbd/paper.pdf) if you're interested.\n",
        "\n",
        "Let's get started by first installing some required packages."
      ],
      "metadata": {
        "id": "CI25mk49bkqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Y8HU50Rwk25F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psg_rqWCKWaW"
      },
      "outputs": [],
      "source": [
        "!pip install open3d ipywidgets matplotlib jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "u9kWhEAgKZx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Code for Point Clouds"
      ],
      "metadata": {
        "id": "72TgwF8sk8qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_point_clouds(pcd_list, azim=-60, elev=30):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    color_fns = [plt.cm.viridis, plt.cm.inferno]\n",
        "    for i, pcd in enumerate(pcd_list):\n",
        "      points = np.asarray(pcd.points)\n",
        "      colors = color_fns[i](points[:, 2] / points[:, 2].max())\n",
        "      ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=10, c=colors)\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    ax.view_init(azim=azim, elev=elev)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "W3eC6jLRKz0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Setup\n",
        "\n",
        "For our experiments in this homework, we will be using the famous [Stanford Bunny.](https://graphics.stanford.edu/data/3Dscanrep/)"
      ],
      "metadata": {
        "id": "N11r4RSGlDyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_bunny = o3d.io.read_point_cloud(\"bun_zipper.ply\")"
      ],
      "metadata": {
        "id": "3uDBsnW1Q7va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `pcd` refers to \"point cloud data\"\n",
        "# Our goal throughout this notebook will be to match the point cloud data\n",
        "# containing a partial view of the bunny\n",
        "# with the full 3D mesh model of the same bunny\n",
        "pcd = o3d.io.read_point_cloud(\"bun045.ply\")"
      ],
      "metadata": {
        "id": "Gy_PttvBKlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pcd is an open3d data structure with different attributes\n",
        "# You can access the 3D points representing the lidar data like this:\n",
        "pcd_np = np.asarray(pcd.points)\n",
        "print(pcd_np.shape)"
      ],
      "metadata": {
        "id": "WOyRHxpPNaY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the full bunny mesh model. Feel free to use the slider bars to view the bunny from different angles."
      ],
      "metadata": {
        "id": "MmWYqsgSmJwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([full_bunny]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "f2WUbOx0RCZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now visualize the partial point cloud data. Again, feel free to use the sliders. You should be able to see that this scan is not a complete scan of the bunny. We will be referring to this partial point cloud data as the \"source\", and the full scan of the bunny as the \"target\".\n",
        "\n",
        "The goal therefore will be to match points in the source to the target. Mathematically, we can say that our overarching goal is to find a matrix T, such that\n",
        "\n",
        "$P_{source} T = P_{target}$\n",
        "\n",
        "where $P_i$ represents a matrix of the point cloud data"
      ],
      "metadata": {
        "id": "OcHc5UYymVAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([pcd]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "vbzGFsKblJ6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Registration\n",
        "\n",
        "Before we can use the ICP algorithm, we need to generate an initial transformation that can represents our best guess for registering the two point clouds. We will do this using the RANSAC algorithm.\n",
        "\n",
        "**You do not have to write any code for this part of the problem. You are highly encouraged to follow along to gain a better understanding of the process.**"
      ],
      "metadata": {
        "id": "vZXSB5vjRgB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's visualize both the source and target point clouds together so that we can see whether they overlap or not."
      ],
      "metadata": {
        "id": "zcerDnMn1kE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_point_clouds(pcd_list, azim=-60, elev=30):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    color_fns = [plt.cm.viridis, plt.cm.inferno]\n",
        "    for i, pcd in enumerate(pcd_list):\n",
        "      points = np.asarray(pcd.points)\n",
        "      colors = color_fns[i](points[:, 2] / points[:, 2].max())\n",
        "      ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=10, c=colors)\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    ax.view_init(azim=azim, elev=elev)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive visualization in Jupyter Notebook using ipywidgets\n",
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([full_bunny, pcd]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "gK-od6izVCH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the two point clouds are clearly misaligned right now. So first, we will extract some features of each point cloud. These features, called the \"FPFH\" features of the point clouds are a vector of 33 values for each point in the point cloud that represents some unique features of that point. Therefore, if you have N points in your point cloud, your FPFH feature matrix for that point cloud will be of shape (N, 33).\n",
        "\n",
        "Since N can be really large for raw point cloud data, we will downsample it a bit so that it is easier to experiment with."
      ],
      "metadata": {
        "id": "xvm9cu6O2JO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features\n",
        "def preprocess_point_cloud(pcd, voxel_size):\n",
        "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
        "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
        "\n",
        "    radius_normal = voxel_size * 2\n",
        "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
        "    pcd_down.estimate_normals(\n",
        "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
        "\n",
        "    radius_feature = voxel_size * 5\n",
        "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
        "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
        "        pcd_down,\n",
        "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
        "    return pcd_down, pcd_fpfh"
      ],
      "metadata": {
        "id": "01_HzKZvRxTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(source, target, voxel_size):\n",
        "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
        "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
        "    return source, target, source_down, target_down, source_fpfh, target_fpfh"
      ],
      "metadata": {
        "id": "w_S112KuR0lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(pcd, full_bunny, 0.01)"
      ],
      "metadata": {
        "id": "iVGRWKN4SBh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block will run the RANSAC algorithm using the open3d library. You can see that the function returns a transformation matrix, which we can apply to our source point cloud data."
      ],
      "metadata": {
        "id": "vUhS9oth2ujI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_global_registration(source_down, target_down, source_fpfh,\n",
        "                                target_fpfh, voxel_size):\n",
        "    distance_threshold = voxel_size * 1.5\n",
        "    print(\":: RANSAC registration on downsampled point clouds.\")\n",
        "    print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
        "    print(\"   we use a liberal distance threshold %.3f.\" % distance_threshold)\n",
        "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
        "        source_down, target_down, source_fpfh, target_fpfh, False, distance_threshold)\n",
        "    return result.transformation"
      ],
      "metadata": {
        "id": "whnzFKQYRhYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size=0.01)"
      ],
      "metadata": {
        "id": "8SEKZCtLRfsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T shape:\", T.shape)\n",
        "T"
      ],
      "metadata": {
        "id": "UMXshIwN2728"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After RANSAC"
      ],
      "metadata": {
        "id": "NEPe_fvQWb0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now apply this transformation matrix to our point cloud data to see how well the point clouds are now registered."
      ],
      "metadata": {
        "id": "KnvL2Mex3nzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "lidar_transformed = deepcopy(pcd).transform(T)"
      ],
      "metadata": {
        "id": "n1bEhie_Ss1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([full_bunny, lidar_transformed]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "i1hHaClNVzdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, they are much better aligned than before but need fine-tuning.We will now use ICP to try to improve this alignment, a process also called \"Local Refinement\""
      ],
      "metadata": {
        "id": "8sih_ecB30Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic ICP\n",
        "\n",
        "In this section, we will implement a basic version of the ICP algorithm. The ICP algorithm has the following steps:\n",
        "\n",
        "\n",
        "\n",
        "1.   For every point in the source point cloud, find its nearest neighbor in the target point cloud\n",
        "2.   Find a matrix T that maps points in the source point cloud to the target point cloud while minimizing the euclidean distance between a point and its nearest neighbor (this will be our **error** for each point)\n",
        "3. Apply this transformation, T, to the source point cloud data\n",
        "4. Compute the average error for this transformation across all points in the source point cloud data\n",
        "5. Repeat steps 1 - 4 for N iterations, or break if:\n",
        "\n",
        "$abs(e_t - e_{t-1}) < \\tau$\n",
        "\n",
        "where $e_i$ is the average error in the i-th iteration and tau is the tolerance set as a hyperparameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "WdMMNWhIcd1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import cv2\n",
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "djwCoW1zK4QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def best_fit_transform(A, B):\n",
        "    '''\n",
        "    Calculates the least-squares best-fit transform that maps corresponding points A to B in m spatial dimensions\n",
        "    Input:\n",
        "      A: Naxm numpy array of corresponding points\n",
        "      B: Nbxm numpy array of corresponding points\n",
        "    Returns:\n",
        "      T: (m+1)x(m+1) homogeneous transformation matrix that maps A on to B\n",
        "      R: mxm rotation matrix\n",
        "      t: mx1 translation vector\n",
        "    '''\n",
        "    # get number of dimensions\n",
        "    m = A.shape[1]\n",
        "\n",
        "    # translate points to their centroids\n",
        "    centroid_A = np.mean(A, axis=0)\n",
        "    centroid_B = np.mean(B, axis=0)\n",
        "    AA = A - centroid_A\n",
        "    BB = B - centroid_B\n",
        "\n",
        "    # rotation matrix\n",
        "    H = np.dot(AA.T, BB)\n",
        "    U, S, Vt = np.linalg.svd(H)\n",
        "    R = np.dot(Vt.T, U.T)\n",
        "\n",
        "    # special reflection case\n",
        "    if np.linalg.det(R) < 0:\n",
        "       Vt[m-1,:] *= -1\n",
        "       R = np.dot(Vt.T, U.T)\n",
        "\n",
        "    # translation\n",
        "    t = centroid_B.T - np.dot(R,centroid_A.T)\n",
        "\n",
        "    # homogeneous transformation\n",
        "    T = np.identity(m+1)\n",
        "    T[:m, :m] = R\n",
        "    T[:m, m] = t\n",
        "\n",
        "    return T, R, t\n",
        "\n",
        "\n",
        "def nearest_neighbor(src, dst, radius=0.01):\n",
        "    '''\n",
        "    Find the nearest (Euclidean) neighbor in dst for each point in src\n",
        "    Input:\n",
        "        src: Nxm array of points\n",
        "        dst: Nxm array of points\n",
        "    Output:\n",
        "        distances: Euclidean distances of the nearest neighbor\n",
        "        indices: dst indices of the nearest neighbor\n",
        "    '''\n",
        "    ######################################################################\n",
        "    ######################### YOUR CODE HERE #############################\n",
        "    # Use the NearestNeighbors class sklearn.neighbors to compute the nearest\n",
        "    # neighbor of each point in the source dataset to the target dataset\n",
        "    # Note that by using this class and its methods correctly, you should be\n",
        "    # able to get the distance between a point and its nearest neighbor,\n",
        "    # as well as the indices of the nearest neighbor in the target point cloud\n",
        "\n",
        "    ######################################################################\n",
        "    return distances.ravel(), indices.ravel()\n",
        "\n",
        "\n",
        "def icp(A, B, init_pose=None, max_iterations=20, tolerance=0.001, knn_radius=0.01):\n",
        "    '''\n",
        "    The Iterative Closest Point method: finds best-fit transform that maps points A on to points B\n",
        "    Input:\n",
        "        A: Nxm numpy array of source mD points\n",
        "        B: Nxm numpy array of destination mD point\n",
        "        init_pose: (m+1)x(m+1) homogeneous transformation\n",
        "        max_iterations: exit algorithm after max_iterations\n",
        "        tolerance: convergence criteria\n",
        "    Output:\n",
        "        T: final homogeneous transformation that maps A on to B\n",
        "        distances: Euclidean distances (errors) of the nearest neighbor\n",
        "        i: number of iterations to converge\n",
        "    '''\n",
        "    # get number of dimensions\n",
        "    m = A.shape[1]\n",
        "\n",
        "    # make points homogeneous, copy them to maintain the originals\n",
        "    src = np.ones((m+1,A.shape[0]))\n",
        "    dst = np.ones((m+1,B.shape[0]))\n",
        "    src[:m,:] = np.copy(A.T)\n",
        "    dst[:m,:] = np.copy(B.T)\n",
        "\n",
        "    # apply the initial pose estimation\n",
        "    if init_pose is not None:\n",
        "        src = np.dot(init_pose, src)\n",
        "\n",
        "    prev_error = 0\n",
        "    ######################################################################\n",
        "    ######################### YOUR CODE HERE #############################\n",
        "    # Write the loop for the ICP algorithm here\n",
        "    # Follow the steps outlined in the prompt above.\n",
        "    # Hints:\n",
        "    # - Use the functions `nearest_neighbor()` and `best_fit_transform()`\n",
        "    # - src and dst matrices are defined above with shape (m+1,N), while the above\n",
        "    #   two function expect matrices of shape (N,m)\n",
        "    ######################################################################\n",
        "    # calculate final transformation\n",
        "    T,_,_ = best_fit_transform(A, src[:m,:].T)\n",
        "\n",
        "    return T"
      ],
      "metadata": {
        "id": "P53SvRafYl-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_np = np.asarray(source_down.points)\n",
        "target_np = np.asarray(target_down.points)\n",
        "\n",
        "T_icp = icp(source_np, target_np, T, max_iterations=20, tolerance=1e-5, knn_radius=1e-2)"
      ],
      "metadata": {
        "id": "P8fK-0A3ePxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcd_down_ICP = deepcopy(source_down).transform(T_icp) # transformed downsampled pcd\n",
        "pcd_ICP = deepcopy(source).transform(T_icp) # transformed complete size pcd"
      ],
      "metadata": {
        "id": "vi54fkA1eUyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your implementation of ICP is correct, you should see that the point clouds roughly align. The source point cloud can seem invisible, which means that it can be aligned to be inside the target point cloud, which is acceptable at this stage. This happens because we implemented a very basic version of ICP that is not robust to noise, outliers, etc."
      ],
      "metadata": {
        "id": "Skkl3UD0lsnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([full_bunny, pcd_ICP]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "ejTVdaIfVLds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visualization should be slightly easier to view and you should see some points from the source point cloud along the edges of the bunny"
      ],
      "metadata": {
        "id": "n7fkNtRSmHDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([full_bunny, pcd_down_ICP]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "QGgiQYZ1nDEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust ICP Using open3D"
      ],
      "metadata": {
        "id": "hrR5onrunNnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use the open3D library's functions to use a robust version of the ICP algorithm. You do not have to write the ICP algorithm itself, but you will only be using library functions from open3D.\n",
        "\n",
        "Follow [this tutorial](http://www.open3d.org/docs/release/tutorial/pipelines/icp_registration.html#Point-to-point-ICP) to see which functions to call and how to use them. You should be calling the ICP registration method, use the point-to-point transformation estimation method, and include a convergence critera to set a max number of iterations. Everything should be available in this same tutorial, you shouldn't have to google or find anything else."
      ],
      "metadata": {
        "id": "UmsKV2aNmPZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.01\n",
        "max_iterations=3000"
      ],
      "metadata": {
        "id": "SGnpp95ddGkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Apply point-to-point ICP\")\n",
        "######################################################################\n",
        "######################### YOUR CODE HERE #############################\n",
        "\n",
        "######################################################################\n",
        "print(reg_p2p)\n",
        "print(\"Transformation is:\")\n",
        "print(reg_p2p.transformation)"
      ],
      "metadata": {
        "id": "oSGo6iBGWWhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcd_robust_ICP = deepcopy(pcd).transform(reg_p2p.transformation)"
      ],
      "metadata": {
        "id": "SGkOTx0fW4uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widgets.interact(\n",
        "    plot_point_clouds,\n",
        "    pcd_list=widgets.fixed([full_bunny, pcd_robust_ICP]),\n",
        "    azim=widgets.IntSlider(-90, min=-180, max=180, step=5, description=\"Azimuth\"),\n",
        "    elev=widgets.IntSlider(90, min=0, max=90, step=5, description=\"Elevation\")\n",
        ")"
      ],
      "metadata": {
        "id": "ZWYmRsURXbi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visualization should now look very well aligned. If done correctly, you should see the source and target point clouds mixing with each other a bit."
      ],
      "metadata": {
        "id": "EplgFLuendfU"
      }
    }
  ]
}
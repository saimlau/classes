
@article{bazi_vision_2021,
	title = {Vision Transformers for Remote Sensing Image Classification},
	volume = {13},
	doi = {10.3390/rs13030516},
	abstract = {In this paper, we propose a remote-sensing scene-classification method based on vision transformers. These types of networks, which are now recognized as state-of-the-art models in natural language processing, do not rely on convolution layers as in standard convolutional neural networks ({CNNs}). Instead, they use multihead attention mechanisms as the main building block to derive long-range contextual relation between pixels in images. In a first step, the images under analysis are divided into patches, then converted to sequence by flattening and embedding. To keep information about the position, embedding position is added to these patches. Then, the resulting sequence is fed to several multihead attention layers for generating the final representation. At the classification stage, the first token sequence is fed to a softmax classification layer. To boost the classification performance, we explore several data augmentation strategies to generate additional data for training. Moreover, we show experimentally that we can compress the network by pruning half of the layers while keeping competing classification accuracies. Experimental results conducted on different remote-sensing image datasets demonstrate the promising capability of the model compared to state-of-the-art methods. Specifically, Vision Transformer obtains an average classification accuracy of 98.49\%, 95.86\%, 95.56\% and 93.83\% on Merced, {AID}, Optimal31 and {NWPU} datasets, respectively. While the compressed version obtained by removing half of the multihead attention layers yields 97.90\%, 94.27\%, 95.30\% and 93.05\%, respectively.},
	pages = {516},
	journaltitle = {Remote Sensing},
	shortjournal = {Remote Sensing},
	author = {Bazi, Yakoub and Bashmal, Laila and Al Rahhal, Mohamad and Dayil, Reham and Ajlan, Naif},
	date = {2021-02-01},
	file = {Full Text PDF:/home/saimai/Zotero/storage/FMGYC8DQ/Bazi et al. - 2021 - Vision Transformers for Remote Sensing Image Classification.pdf:application/pdf},
}

@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {{arXiv}:2010.11929},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2024-11-09},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/saimai/Zotero/storage/PIK7FC92/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:application/pdf;Snapshot:/home/saimai/Zotero/storage/YEMEWDFQ/2010.html:text/html},
}

@online{pulfer_vision_2024,
	title = {Vision Transformers from Scratch ({PyTorch}): A step-by-step guide},
	url = {https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c},
	shorttitle = {Vision Transformers from Scratch ({PyTorch})},
	abstract = {Vision Transformers ({ViT}), since their introduction by Dosovitskiy et. al. [reference] in 2020, have dominated the field of Computer…},
	titleaddon = {Medium},
	author = {Pulfer, Brian},
	urldate = {2024-11-10},
	date = {2024-03-21},
	langid = {english},
	file = {Snapshot:/home/saimai/Zotero/storage/KANXS2I9/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c.html:text/html},
}

@online{noauthor_practical_nodate,
	title = {Practical {ML} Dive - How to customize a Vision Transformer on your own data {\textbar} Oxen.ai},
	url = {https://ghost.oxen.ai/practical-ml-dive-how-to-customize-a-vision-transformer-on-your-own-data/},
	abstract = {Welcome to Practical {ML} Dives, a series spin off of Arxiv Dives.

In Arxiv Dives, we cover state of the art research papers, and dive into the gnitty gritty details of how {AI} models work. From the math to the data to the model architecture, we cover it all.

Arxiv Dives - Oxen.{aiEach} week we dive deep into a topic in machine learning, data management, or general artificial intelligence research. These are notes from a live reading group we do every Friday. Captured for future reference.Oxen.ai},
	urldate = {2024-11-10},
	langid = {english},
	file = {Snapshot:/home/saimai/Zotero/storage/N2B8ZH4K/practical-ml-dive-how-to-customize-a-vision-transformer-on-your-own-data.html:text/html},
}

@misc{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771},
	doi = {10.48550/arXiv.1910.03771},
	shorttitle = {{HuggingFace}'s Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	number = {{arXiv}:1910.03771},
	publisher = {{arXiv}},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	urldate = {2024-11-10},
	date = {2020-07-14},
	eprinttype = {arxiv},
	eprint = {1910.03771},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/saimai/Zotero/storage/6GRVDK4Q/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natural Language Processing.pdf:application/pdf;Snapshot:/home/saimai/Zotero/storage/HRBN2K4D/1910.html:text/html},
}

@inproceedings{li_relationship_2021,
	title = {Relationship between learning engagement metrics and learning outcomes in online engineering course},
	url = {https://ieeexplore.ieee.org/document/9637234},
	doi = {10.1109/FIE49875.2021.9637234},
	abstract = {This research {WIP} contributes to understanding the relationship between learning engagement in Learning Management System ({LMS}) and outcomes in an online course. In large engineering courses, it is challenging for instructors to identify who is engaging with course materials at a level necessary to be successful in terms of course outcomes. The purpose of this research {WIP} study is two-fold: (1) to develop metrics for quantifying learner engagement in online courses, and (2) to explore the relationship between engagement and student success. Our research question is: How does learning engagement relate to course outcomes? We modeled learner engagement on a course level using the following features: number of views per content object, total time spent in the platform, percentage of the course accessed by the learners, percentage of feedback read, and number of attempts per quiz. We used the data collected by the {LMS} in a large first-year engineering course. We obtained data in Fall 2020, the first semester that many traditional universities were forced mostly or entirely online. After calculating the proposed metrics, we used a linear mixed model to analyze the effect of engagement on learning outcomes. Our linear mixed model shows that all engagement metrics are positively related to the final grade. However, the results also indicate that the relationship between engagement and learning outcomes is not linear; more complex modeling is needed to further explore this relationship.},
	eventtitle = {2021 {IEEE} Frontiers in Education Conference ({FIE})},
	pages = {1--5},
	booktitle = {2021 {IEEE} Frontiers in Education Conference ({FIE})},
	author = {Li, Tiantian and Castro, Laura Melissa Cruz and Douglas, Kerrie and Brinton, Christopher G.},
	urldate = {2024-12-06},
	date = {2021-10},
	note = {{ISSN}: 2377-634X},
	keywords = {Analytical models, Conferences, generalized linear mixed model, learning engagement, Learning management systems, learning outcome, Measurement, Monitoring, online learning, Real-time systems, Visualization},
	file = {Full Text PDF:/home/saimai/Zotero/storage/TZH7ZREQ/Li et al. - 2021 - Relationship between learning engagement metrics and learning outcomes in online engineering course.pdf:application/pdf;IEEE Xplore Abstract Record:/home/saimai/Zotero/storage/4FJV4JPL/9637234.html:text/html},
}

@misc{arnab_vivit_2021,
	title = {{ViViT}: A Video Vision Transformer},
	url = {http://arxiv.org/abs/2103.15691},
	doi = {10.48550/arXiv.2103.15691},
	shorttitle = {{ViViT}},
	abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit},
	number = {{arXiv}:2103.15691},
	publisher = {{arXiv}},
	author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
	urldate = {2024-12-06},
	date = {2021-11-01},
	eprinttype = {arxiv},
	eprint = {2103.15691 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/saimai/Zotero/storage/NT2J2LF9/Arnab et al. - 2021 - ViViT A Video Vision Transformer.pdf:application/pdf;Snapshot:/home/saimai/Zotero/storage/3H89QK2Z/2103.html:text/html},
}

@misc{tong_videomae_2022,
	title = {{VideoMAE}: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
	url = {http://arxiv.org/abs/2203.12602},
	doi = {10.48550/arXiv.2203.12602},
	shorttitle = {{VideoMAE}},
	abstract = {Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders ({VideoMAE}) are data-efficient learners for self-supervised video pre-training ({SSVP}). We are inspired by the recent {ImageMAE} and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on {SSVP}: (1) An extremely high proportion of masking ratio (i.e., 90\% to 95\%) still yields favorable performance of {VideoMAE}. The temporally redundant video content enables a higher masking ratio than that of images. (2) {VideoMAE} achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) {VideoMAE} shows that data quality is more important than data quantity for {SSVP}. Domain shift between pre-training and target datasets is an important issue. Notably, our {VideoMAE} with the vanilla {ViT} can achieve 87.4\% on Kinetics-400, 75.4\% on Something-Something V2, 91.3\% on {UCF}101, and 62.6\% on {HMDB}51, without using any extra data. Code is available at https://github.com/{MCG}-{NJU}/{VideoMAE}.},
	number = {{arXiv}:2203.12602},
	publisher = {{arXiv}},
	author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
	urldate = {2024-12-06},
	date = {2022-10-18},
	eprinttype = {arxiv},
	eprint = {2203.12602 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/saimai/Zotero/storage/85C48ZKP/Tong et al. - 2022 - VideoMAE Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training.pdf:application/pdf;Snapshot:/home/saimai/Zotero/storage/FXLZM2GT/2203.html:text/html},
}
